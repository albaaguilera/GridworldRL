# GridworldRL
Single-agent and Multi-agent Reinforcement Learning in a simple Grid-world environment. 

Our agent, represented as a blue dot, needs to explore the environment to find a target located randomly each time. By trial and error, the agent finds the most optimal way to find the target.

Compared **Value Iteration** (known transitions and rewards: model-based) with **Q-learning** (unknown transitions and rewards: model-free).

| **Feature**                         | **Value Iteration**                                                                                                                                     | **Q-learning**                                                                                          |
|-------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| Needs full knowledge of the environment? | ‚úÖ Yes                                                                                                                                              | ‚ùå No                                                                                                     |
| Exploration required?               | ‚ùå No                                                                                                                                               | ‚úÖ Yes (e.g., Œµ-greedy)                                                                                  |
| Algorithm                           | Bellman equation updates over all states                                         | Trial-and-error with q-table updates over all states|
| Convergence                         | Fast & deterministic                                                                                                                   | Slower, stochastic                                                                                       |
| When to use                         | Known small MDPs                                                                                                                                       | Large or unknown environments                                                                            |

---

## üé• Single-Agent Behavior

<table>
<tr>
<td align="center"><strong>Value Iteration</strong><br><img src="gifs/value_iteration.gif" width="300"/></td>
<td align="center"><strong>Q-learning</strong><br><img src="gifs/qlearning.gif" width="300"/></td>
</tr>
</table>


## üé• Multi-Agent Behavior

<table>
<tr>
<td align="center"><strong>Value Iteration</strong><br><img src="gifs/value_iteration.gif" width="300"/></td>
<td align="center"><strong>Q-learning</strong><br><img src="gifs/qlearning_summary.gif" width="300"/></td>
</tr>
</table>


## üß™ Training Results

![Training Results](data/training_results.png)